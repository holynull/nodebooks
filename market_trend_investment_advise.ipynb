{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "491d6fed",
   "metadata": {},
   "source": [
    "## ContentHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ce1fd3-73c1-44ab-8087-7c523352f8bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "import json\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:\n",
    "        input_str = json.dumps({\"inputs\": prompt, **model_kwargs})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ff1caf",
   "metadata": {},
   "source": [
    "## IndicatorsQuestionsChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d2a640",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "from pydantic import Extra\n",
    "\n",
    "from langchain.base_language import BaseLanguageModel\n",
    "from langchain.callbacks.manager import (\n",
    "    AsyncCallbackManagerForChainRun,\n",
    "    CallbackManagerForChainRun,\n",
    ")\n",
    "from langchain.chains.base import Chain\n",
    "from langchain.prompts.base import BasePromptTemplate\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "class IndicatorsQuestionsChain(Chain):\n",
    "    \"\"\"\n",
    "    An example of a custom chain.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt: BasePromptTemplate\n",
    "    \"\"\"Prompt object to use.\"\"\"\n",
    "    llm: BaseLanguageModel\n",
    "    output_key: str = \"text\"  #: :meta private:\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
    "\n",
    "        extra = Extra.forbid\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        \"\"\"Will be whatever keys the prompt expects.\n",
    "\n",
    "        :meta private:\n",
    "        \"\"\"\n",
    "        return self.prompt.input_variables\n",
    "\n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        \"\"\"Will always return text key.\n",
    "\n",
    "        :meta private:\n",
    "        \"\"\"\n",
    "        return [self.output_key]\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        inputs: Dict[str, Any],\n",
    "        run_manager: Optional[CallbackManagerForChainRun] = None,\n",
    "    ) -> Dict[str, str]:\n",
    "        # Your custom chain logic goes here\n",
    "        # This is just an example that mimics LLMChain\n",
    "        prompt_value = self.prompt.format_prompt(**inputs)\n",
    "        if run_manager:\n",
    "            run_manager.on_text(\n",
    "                prompt_value.to_string(), color=\"green\", end=\"\\n\", verbose=self.verbose\n",
    "            )\n",
    "        # Whenever you call a language model, or another chain, you should pass\n",
    "        # a callback manager to it. This allows the inner run to be tracked by\n",
    "        # any callbacks that are registered on the outer run.\n",
    "        # You can always obtain a callback manager for this by calling\n",
    "        # `run_manager.get_child()` as shown below.\n",
    "        response = self.llm.generate_prompt(\n",
    "            [prompt_value], callbacks=run_manager.get_child() if run_manager else None\n",
    "        )\n",
    "\n",
    "        # If you want to log something about this run, you can do so by calling\n",
    "        # methods on the `run_manager`, as shown below. This will trigger any\n",
    "        # callbacks that are registered for that event.\n",
    "        if run_manager:\n",
    "            run_manager.on_text(\n",
    "                response.generations[0][0].text,\n",
    "                color=\"yellow\",\n",
    "                end=\"\\n\",\n",
    "                verbose=self.verbose,\n",
    "            )\n",
    "        return {self.output_key: response.generations[0][0].text}\n",
    "\n",
    "    async def _acall(\n",
    "        self,\n",
    "        inputs: Dict[str, Any],\n",
    "        run_manager: Optional[AsyncCallbackManagerForChainRun] = None,\n",
    "    ) -> Dict[str, str]:\n",
    "        # Your custom chain logic goes here\n",
    "        # This is just an example that mimics LLMChain\n",
    "        prompt_value = self.prompt.format_prompt(**inputs)\n",
    "        if run_manager:\n",
    "            await run_manager.on_text(\n",
    "                prompt_value.to_string(), color=\"green\", end=\"\\n\", verbose=self.verbose\n",
    "            )\n",
    "        # Whenever you call a language model, or another chain, you should pass\n",
    "        # a callback manager to it. This allows the inner run to be tracked by\n",
    "        # any callbacks that are registered on the outer run.\n",
    "        # You can always obtain a callback manager for this by calling\n",
    "        # `run_manager.get_child()` as shown below.\n",
    "        response = await self.llm.agenerate_prompt(\n",
    "            [prompt_value], callbacks=run_manager.get_child() if run_manager else None\n",
    "        )\n",
    "\n",
    "        # If you want to log something about this run, you can do so by calling\n",
    "        # methods on the `run_manager`, as shown below. This will trigger any\n",
    "        # callbacks that are registered for that event.\n",
    "        if run_manager:\n",
    "            await run_manager.on_text(\n",
    "                response.generations[0][0].text,\n",
    "                color=\"yellow\",\n",
    "                end=\"\\n\",\n",
    "                verbose=self.verbose,\n",
    "            )\n",
    "        return {self.output_key: response.generations[0][0].text}\n",
    "\n",
    "    @property\n",
    "    def _chain_type(self) -> str:\n",
    "        return \"indicators_questions_chain\"\n",
    "\n",
    "    @classmethod\n",
    "    def from_indicators(\n",
    "        cls,\n",
    "        indicators: str,\n",
    "        **kwargs: Any,\n",
    "    ) -> IndicatorsQuestionsChain:\n",
    "        PROMPT_TEMPLATE = (\n",
    "            \"\"\"The user's question may be to ask the latest market trend of a certain cryptocurrency. \n",
    "The following index tools can help users analyze market trend.\n",
    "Index tool's names are \"\"\"\n",
    "            + indicators\n",
    "        )\n",
    "        PROMPT_TEMPLATE = (\n",
    "            PROMPT_TEMPLATE\n",
    "            + \"\"\"\\nPlease generats questions, to ask the latest index above of the cryptocurrency with its symbol in user's question.\n",
    "Note that some index are composed of a set of numerical values. So you should generate the questions ask to get all values of the index.\n",
    "You should generate the questions in JSON Object format, and use index tool's name as JSON object's key, use the question which you generated as the value of JSON object.\n",
    "Do not need any hint, just a JSON object.\n",
    "User's Question: {question}\n",
    "You generations:\"\"\"\n",
    "        )\n",
    "        # Finally, add a item into the JSON object, key is \"question\", use the following user's question as its value.\n",
    "\n",
    "        prompt = PromptTemplate(input_variables=[\"question\"], template=PROMPT_TEMPLATE)\n",
    "        content_handler = ContentHandler()\n",
    "        llm = SagemakerEndpoint(\n",
    "            endpoint_name=\"huggingface-pytorch-tgi-inference-2023-07-24-07-23-15-934\",\n",
    "            # credentials_profile_name=\"default\",\n",
    "            region_name=\"us-east-1\",\n",
    "            model_kwargs={\n",
    "                \"parameters\": {\n",
    "                    \"do_sample\": True,\n",
    "                    # \"top_p\": 0.9,\n",
    "                    # \"top_k\": 10,\n",
    "                    \"repetition_penalty\": 1.03,\n",
    "                    \"max_new_tokens\": 1024,\n",
    "                    \"temperature\": 0.1,\n",
    "                    \"return_full_text\": False,\n",
    "                    # \"max_length\":2048,\n",
    "                    \"truncate\": 2048,\n",
    "                    # \"num_return_sequences\":2000,\n",
    "                    # \"stop\": [\"\\n\"],\n",
    "                },\n",
    "            },\n",
    "            content_handler=content_handler,\n",
    "            verbose=kwargs[\"verbose\"],\n",
    "        )\n",
    "        return cls(llm=llm, prompt=prompt, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f7c775",
   "metadata": {},
   "source": [
    "## Llama2APIChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e897e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import APIChain\n",
    "from langchain import LLMChain\n",
    "from langchain.requests import TextRequestsWrapper\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import SagemakerEndpoint\n",
    "\n",
    "\n",
    "class Llama2APIChain(APIChain):\n",
    "    @property\n",
    "    def _chain_type(self) -> str:\n",
    "        return \"llama2_api_chain\"\n",
    "\n",
    "    @classmethod\n",
    "    def from_docs(\n",
    "        cls,\n",
    "        api_docs: str,\n",
    "        headers: any,\n",
    "        **kwargs: any,\n",
    "    ) -> Llama2APIChain:\n",
    "        content_handler = ContentHandler()\n",
    "\n",
    "        llm_gen_url = SagemakerEndpoint(\n",
    "            endpoint_name=\"huggingface-pytorch-tgi-inference-2023-07-24-07-23-15-934\",\n",
    "            # credentials_profile_name=\"default\",\n",
    "            region_name=\"us-east-1\",\n",
    "            model_kwargs={\n",
    "                \"parameters\": {\n",
    "                    \"do_sample\": True,\n",
    "                    # \"top_p\": 0.9,\n",
    "                    # \"top_k\": 10,\n",
    "                    \"repetition_penalty\": 1.03,\n",
    "                    \"max_new_tokens\": 1024,\n",
    "                    \"temperature\": 0.1,\n",
    "                    \"return_full_text\": False,\n",
    "                    # \"max_length\":2048,\n",
    "                    \"truncate\": 2048,\n",
    "                    # \"num_return_sequences\":2000,\n",
    "                    # \"stop\": [\"\\n\"],\n",
    "                },\n",
    "            },\n",
    "            content_handler=content_handler,\n",
    "            verbose=kwargs[\"verbose\"],\n",
    "        )\n",
    "\n",
    "        llm_gen_resp = SagemakerEndpoint(\n",
    "            endpoint_name=\"huggingface-pytorch-tgi-inference-2023-07-24-07-23-15-934\",\n",
    "            # credentials_profile_name=\"default\",\n",
    "            region_name=\"us-east-1\",\n",
    "            model_kwargs={\n",
    "                \"parameters\": {\n",
    "                    \"do_sample\": True,\n",
    "                    \"top_p\": 0.9,\n",
    "                    \"top_k\": 10,\n",
    "                    \"repetition_penalty\": 1.03,\n",
    "                    \"max_new_tokens\": 1024,\n",
    "                    \"temperature\": 0.9,\n",
    "                    \"return_full_text\": False,\n",
    "                    # \"max_length\":2048,\n",
    "                    \"truncate\": 2048,\n",
    "                    # \"num_return_sequences\":2000,\n",
    "                    # \"stop\": [\"\\nHuman:\"],\n",
    "                },\n",
    "            },\n",
    "            content_handler=content_handler,\n",
    "            verbose=kwargs[\"verbose\"],\n",
    "        )\n",
    "\n",
    "        API_URL_PROMPT_TEMPLATE = \"\"\"API Documentation:\n",
    "{api_docs}\n",
    "\n",
    "According above documentation, the full API url was generated to call for answering the question bellow.\n",
    "The full API url in order to get a response that is as short as possible. And the full API url to deliberately exclude any unnecessary pieces of data in the API call.\n",
    "You should not generate any hint. Just generate the full API url to answer the question as far as possible.\n",
    "\n",
    "Question:{question}\n",
    "\n",
    "API url:\"\"\"\n",
    "        API_URL_PROMPT = PromptTemplate(\n",
    "            input_variables=[\n",
    "                \"api_docs\",\n",
    "                \"question\",\n",
    "            ],\n",
    "            template=API_URL_PROMPT_TEMPLATE,\n",
    "        )\n",
    "        API_RESPONSE_PROMPT_TEMPLATE = (\n",
    "            API_URL_PROMPT_TEMPLATE\n",
    "            + \"\"\" {api_url}\n",
    "    \n",
    "Here is the response from the API:\n",
    "\n",
    "{api_response}\n",
    "\n",
    "Summarize this response to answer the original question.\n",
    "Please include all data in the response.\n",
    "\n",
    "Summary:\"\"\"\n",
    "        )\n",
    "        API_RESPONSE_PROMPT = PromptTemplate.from_template(API_RESPONSE_PROMPT_TEMPLATE)\n",
    "        return cls(\n",
    "            api_request_chain=LLMChain(\n",
    "                llm=llm_gen_url, prompt=API_URL_PROMPT, **kwargs\n",
    "            ),\n",
    "            api_answer_chain=LLMChain(\n",
    "                llm=llm_gen_resp, prompt=API_RESPONSE_PROMPT, **kwargs\n",
    "            ),\n",
    "            api_docs=api_docs,\n",
    "            requests_wrapper=TextRequestsWrapper(headers=headers),\n",
    "            **kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42888fd",
   "metadata": {},
   "source": [
    "## MarketTrendChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d46f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "from pydantic import Extra\n",
    "\n",
    "from langchain.base_language import BaseLanguageModel\n",
    "from langchain.callbacks.manager import (\n",
    "    AsyncCallbackManagerForChainRun,\n",
    "    CallbackManagerForChainRun,\n",
    ")\n",
    "from langchain.chains.base import Chain\n",
    "from langchain.prompts.base import BasePromptTemplate\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "class MarketTrendChain(Chain):\n",
    "    \"\"\"\n",
    "    An example of a custom chain.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt: BasePromptTemplate\n",
    "    \"\"\"Prompt object to use.\"\"\"\n",
    "    llm: BaseLanguageModel\n",
    "    output_key: str = \"text\"  #: :meta private:\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
    "\n",
    "        extra = Extra.forbid\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        \"\"\"Will be whatever keys the prompt expects.\n",
    "\n",
    "        :meta private:\n",
    "        \"\"\"\n",
    "        return self.prompt.input_variables\n",
    "\n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        \"\"\"Will always return text key.\n",
    "\n",
    "        :meta private:\n",
    "        \"\"\"\n",
    "        return [self.output_key]\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        inputs: Dict[str, Any],\n",
    "        run_manager: Optional[CallbackManagerForChainRun] = None,\n",
    "    ) -> Dict[str, str]:\n",
    "        # Your custom chain logic goes here\n",
    "        # This is just an example that mimics LLMChain\n",
    "        prompt_value = self.prompt.format_prompt(**inputs)\n",
    "        if run_manager:\n",
    "            run_manager.on_text(\n",
    "                prompt_value.to_string(), color=\"green\", end=\"\\n\", verbose=self.verbose\n",
    "            )\n",
    "        # Whenever you call a language model, or another chain, you should pass\n",
    "        # a callback manager to it. This allows the inner run to be tracked by\n",
    "        # any callbacks that are registered on the outer run.\n",
    "        # You can always obtain a callback manager for this by calling\n",
    "        # `run_manager.get_child()` as shown below.\n",
    "        response = self.llm.generate_prompt(\n",
    "            [prompt_value], callbacks=run_manager.get_child() if run_manager else None\n",
    "        )\n",
    "\n",
    "        # If you want to log something about this run, you can do so by calling\n",
    "        # methods on the `run_manager`, as shown below. This will trigger any\n",
    "        # callbacks that are registered for that event.\n",
    "        if run_manager:\n",
    "            run_manager.on_text(\n",
    "                response.generations[0][0].text,\n",
    "                color=\"yellow\",\n",
    "                end=\"\\n\",\n",
    "                verbose=self.verbose,\n",
    "            )\n",
    "        return {self.output_key: response.generations[0][0].text}\n",
    "\n",
    "    async def _acall(\n",
    "        self,\n",
    "        inputs: Dict[str, Any],\n",
    "        run_manager: Optional[AsyncCallbackManagerForChainRun] = None,\n",
    "    ) -> Dict[str, str]:\n",
    "        # Your custom chain logic goes here\n",
    "        # This is just an example that mimics LLMChain\n",
    "        prompt_value = self.prompt.format_prompt(**inputs)\n",
    "        if run_manager:\n",
    "            await run_manager.on_text(\n",
    "                prompt_value.to_string(), color=\"green\", end=\"\\n\", verbose=self.verbose\n",
    "            )\n",
    "        # Whenever you call a language model, or another chain, you should pass\n",
    "        # a callback manager to it. This allows the inner run to be tracked by\n",
    "        # any callbacks that are registered on the outer run.\n",
    "        # You can always obtain a callback manager for this by calling\n",
    "        # `run_manager.get_child()` as shown below.\n",
    "        response = await self.llm.agenerate_prompt(\n",
    "            [prompt_value], callbacks=run_manager.get_child() if run_manager else None\n",
    "        )\n",
    "\n",
    "        # If you want to log something about this run, you can do so by calling\n",
    "        # methods on the `run_manager`, as shown below. This will trigger any\n",
    "        # callbacks that are registered for that event.\n",
    "        if run_manager:\n",
    "            await run_manager.on_text(\n",
    "                response.generations[0][0].text,\n",
    "                color=\"yellow\",\n",
    "                end=\"\\n\",\n",
    "                verbose=self.verbose,\n",
    "            )\n",
    "        return {self.output_key: response.generations[0][0].text}\n",
    "\n",
    "    @property\n",
    "    def _chain_type(self) -> str:\n",
    "        return \"indicators_questions_chain\"\n",
    "\n",
    "    @classmethod\n",
    "    def from_llm(\n",
    "        cls,\n",
    "        llm: BaseLanguageModel,\n",
    "        **kwargs: Any,\n",
    "    ) -> MarketTrendChain:\n",
    "        PROMPT_TEMPLATE = \"\"\"Content\n",
    "```\n",
    "{data}\n",
    "```\n",
    "\n",
    "Question\n",
    "```\n",
    "{question}\n",
    "```\n",
    "\n",
    "\n",
    "Please organize the above content into a complete paragraph first. Then use the content to answer the question. Finally, give your anlysis from the content, and give some investment advise as far as possible. \n",
    "\n",
    "\"\"\"\n",
    "        prompt = PromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "        return cls(llm=llm, prompt=prompt, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9656426f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from typing import Any, Dict, List, Optional\n",
    "import datetime\n",
    "from pydantic import Extra\n",
    "\n",
    "from langchain.base_language import BaseLanguageModel\n",
    "from langchain.callbacks.manager import (\n",
    "    AsyncCallbackManagerForChainRun,\n",
    "    CallbackManagerForChainRun,\n",
    ")\n",
    "from langchain.chains.base import Chain\n",
    "from langchain.prompts.base import BasePromptTemplate\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "class TradingViewReasearchReportChain(Chain):\n",
    "    \"\"\"\n",
    "    An example of a custom chain.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt: BasePromptTemplate\n",
    "    \"\"\"Prompt object to use.\"\"\"\n",
    "    llm: BaseLanguageModel\n",
    "    output_key: str = \"text\"  #: :meta private:\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
    "\n",
    "        extra = Extra.forbid\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        \"\"\"Will be whatever keys the prompt expects.\n",
    "\n",
    "        :meta private:\n",
    "        \"\"\"\n",
    "        return self.prompt.input_variables\n",
    "\n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        \"\"\"Will always return text key.\n",
    "\n",
    "        :meta private:\n",
    "        \"\"\"\n",
    "        return [self.output_key]\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        inputs: Dict[str, Any],\n",
    "        run_manager: Optional[CallbackManagerForChainRun] = None,\n",
    "    ) -> Dict[str, str]:\n",
    "        # Your custom chain logic goes here\n",
    "        # This is just an example that mimics LLMChain\n",
    "        prompt_value = self.prompt.format_prompt(**inputs)\n",
    "        if run_manager:\n",
    "            run_manager.on_text(\n",
    "                prompt_value.to_string(),\n",
    "                color=\"green\",\n",
    "                end=\"\\n\",\n",
    "                verbose=self.verbose,\n",
    "            )\n",
    "        # Whenever you call a language model, or another chain, you should pass\n",
    "        # a callback manager to it. This allows the inner run to be tracked by\n",
    "        # any callbacks that are registered on the outer run.\n",
    "        # You can always obtain a callback manager for this by calling\n",
    "        # `run_manager.get_child()` as shown below.\n",
    "        response = self.llm.generate_prompt(\n",
    "            [prompt_value], callbacks=run_manager.get_child() if run_manager else None\n",
    "        )\n",
    "\n",
    "        # If you want to log something about this run, you can do so by calling\n",
    "        # methods on the `run_manager`, as shown below. This will trigger any\n",
    "        # callbacks that are registered for that event.\n",
    "        # answer=self.answer_chain.run(question=inputs['user_input'],context=res)\n",
    "        if run_manager:\n",
    "            run_manager.on_text(\n",
    "                response.generations[0][0].text,\n",
    "                color=\"yellow\",\n",
    "                end=\"\\n\",\n",
    "                verbose=self.verbose,\n",
    "            )\n",
    "        return {self.output_key: response.generations[0][0].text}\n",
    "\n",
    "    async def _acall(\n",
    "        self,\n",
    "        inputs: Dict[str, Any],\n",
    "        run_manager: Optional[AsyncCallbackManagerForChainRun] = None,\n",
    "    ) -> Dict[str, str]:\n",
    "        # Your custom chain logic goes here\n",
    "        # This is just an example that mimics LLMChain\n",
    "        prompt_value = self.prompt.format_prompt(**inputs)\n",
    "        if run_manager:\n",
    "            await run_manager.on_text(\n",
    "                prompt_value.to_string(),\n",
    "                color=\"green\",\n",
    "                end=\"\\n\",\n",
    "                verbose=self.verbose,\n",
    "            )\n",
    "        # Whenever you call a language model, or another chain, you should pass\n",
    "        # a callback manager to it. This allows the inner run to be tracked by\n",
    "        # any callbacks that are registered on the outer run.\n",
    "        # You can always obtain a callback manager for this by calling\n",
    "        # `run_manager.get_child()` as shown below.\n",
    "        response = await self.llm.agenerate_prompt(\n",
    "            [prompt_value], callbacks=run_manager.get_child() if run_manager else None\n",
    "        )\n",
    "\n",
    "        # If you want to log something about this run, you can do so by calling\n",
    "        # methods on the `run_manager`, as shown below. This will trigger any\n",
    "        # callbacks that are registered for that event.\n",
    "        if run_manager:\n",
    "            await run_manager.on_text(\n",
    "                response.generations[0][0].text,\n",
    "                color=\"yellow\",\n",
    "                end=\"\\n\",\n",
    "                verbose=self.verbose,\n",
    "            )\n",
    "        return {self.output_key: response.generations[0][0].text}\n",
    "\n",
    "    @property\n",
    "    def _chain_type(self) -> str:\n",
    "        return \"indicators_questions_chain\"\n",
    "\n",
    "    @classmethod\n",
    "    def from_llm(\n",
    "        cls,\n",
    "        llm,\n",
    "        **kwargs: Any,\n",
    "    ) -> TradingViewReasearchReportChain:\n",
    "        time = f\"It is {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')} now.\\n\"\n",
    "        PROMPT_TEMPLATE = (\n",
    "            time\n",
    "            + \"\"\"We have an unofficial python API wrapper to retrieve technical analysis from TradingView.\n",
    "Retrieving the analysis:\n",
    "- summary: Technical analysis (based on both oscillators and moving averages).\n",
    "```\n",
    "# Example\n",
    "{{'RECOMMENDATION': 'BUY', 'BUY': 12, 'SELL': 7, 'NEUTRAL': 9}}\n",
    "```\n",
    "- oscillators: Technical analysis (based on oscillators).\n",
    "```\n",
    "# Example\n",
    "{{'RECOMMENDATION': 'BUY', 'BUY': 2, 'SELL': 1, 'NEUTRAL': 8, 'COMPUTE': {{'RSI': 'NEUTRAL', 'STOCH.K': 'NEUTRAL', 'CCI': 'NEUTRAL', 'ADX': 'NEUTRAL', 'AO': 'NEUTRAL', 'Mom': 'BUY', 'MACD': 'SELL', 'Stoch.RSI': 'NEUTRAL', 'W%R': 'NEUTRAL', 'BBP': 'BUY', 'UO': 'NEUTRAL'}}}}\n",
    "```\n",
    "- moving_averages: Technical analysis (based on moving averages).\n",
    "```\n",
    "# Example\n",
    "{{'RECOMMENDATION': 'BUY', 'BUY': 9, 'SELL': 5, 'NEUTRAL': 1, 'COMPUTE': {{'EMA10': 'SELL', 'SMA10': 'SELL', 'EMA20': 'SELL', 'SMA20': 'SELL', 'EMA30': 'BUY', 'SMA30': 'BUY', 'EMA50': 'BUY', 'SMA50': 'BUY', 'EMA100': 'BUY', 'SMA100': 'BUY', 'EMA200': 'BUY', 'SMA200': 'BUY', 'Ichimoku': 'NEUTRAL', 'VWMA': 'SELL', 'HullMA': 'BUY'}}}}\n",
    "```\n",
    "- indicators: Technical indicators.\n",
    "```\n",
    "# Example\n",
    "{{'Recommend.Other': 0, 'Recommend.All': 0.26666667, 'Recommend.MA': 0.53333333, 'RSI': 60.28037412, 'RSI[1]': 58.58364778, 'Stoch.K': 73.80404453, 'Stoch.D': 79.64297643, 'Stoch.K[1]': 78.88160227, 'Stoch.D[1]': 85.97647064, 'CCI20': 46.58442886, 'CCI20[1]': 34.57058796, 'ADX': 35.78754863, 'ADX+DI': 23.16948389, 'ADX-DI': 13.82449817, 'ADX+DI[1]': 24.15991909, 'ADX-DI[1]': 13.87125505, 'AO': 6675.72158824, 'AO[1]': 7283.92420588, 'Mom': 1532.6, 'Mom[1]': 108.29, 'MACD.macd': 2444.73734978, 'MACD.signal': 2606.00138275, 'Rec.Stoch.RSI': 0, 'Stoch.RSI.K': 18.53740187, 'Rec.WR': 0, 'W.R': -26.05634845, 'Rec.BBPower': 0, 'BBPower': 295.52055898, 'Rec.UO': 0, 'UO': 55.68311917, 'close': 45326.97, 'EMA5': 45600.06414333, 'SMA5': 45995.592, 'EMA10': 45223.22433151, 'SMA10': 45952.635, 'EMA20': 43451.52018338, 'SMA20': 43609.214, 'EMA30': 41908.5944052, 'SMA30': 40880.391, 'EMA50': 40352.10222373, 'SMA50': 37819.3566, 'EMA100': 40356.09177879, 'SMA100': 38009.7808, 'EMA200': 39466.50411569, 'SMA200': 45551.36135, 'Rec.Ichimoku': 0, 'Ichimoku.BLine': 40772.57, 'Rec.VWMA': 1, 'VWMA': 43471.81729377, 'Rec.HullMA9': -1, 'HullMA9': 45470.37107407, 'Pivot.M.Classic.S3': 11389.27666667, 'Pivot.M.Classic.S2': 24559.27666667, 'Pivot.M.Classic.S1': 33010.55333333, 'Pivot.M.Classic.Middle': 37729.27666667, 'Pivot.M.Classic.R1': 46180.55333333, 'Pivot.M.Classic.R2': 50899.27666667, 'Pivot.M.Classic.R3': 64069.27666667, 'Pivot.M.Fibonacci.S3': 24559.27666667, 'Pivot.M.Fibonacci.S2': 29590.21666667, 'Pivot.M.Fibonacci.S1': 32698.33666667, 'Pivot.M.Fibonacci.Middle': 37729.27666667, 'Pivot.M.Fibonacci.R1': 42760.21666667, 'Pivot.M.Fibonacci.R2': 45868.33666667, 'Pivot.M.Fibonacci.R3': 50899.27666667, 'Pivot.M.Camarilla.S3': 37840.08, 'Pivot.M.Camarilla.S2': 39047.33, 'Pivot.M.Camarilla.S1': 40254.58, 'Pivot.M.Camarilla.Middle': 37729.27666667, 'Pivot.M.Camarilla.R1': 42669.08, 'Pivot.M.Camarilla.R2': 43876.33, 'Pivot.M.Camarilla.R3': 45083.58, 'Pivot.M.Woodie.S3': 21706.84, 'Pivot.M.Woodie.S2': 25492.42, 'Pivot.M.Woodie.S1': 34876.84, 'Pivot.M.Woodie.Middle': 38662.42, 'Pivot.M.Woodie.R1': 48046.84, 'Pivot.M.Woodie.R2': 51832.42, 'Pivot.M.Woodie.R3': 61216.84, 'Pivot.M.Demark.S1': 35369.915, 'Pivot.M.Demark.Middle': 38908.9575, 'Pivot.M.Demark.R1': 48539.915, 'open': 44695.95, 'P.SAR': 48068.64, 'BB.lower': 37961.23510877, 'BB.upper': 49257.19289123, 'AO[2]': 7524.31223529, 'volume': 32744.424503, 'change': 1.44612354, 'low': 44203.28, 'high': 45560}}\n",
    "```\n",
    "\n",
    "We got the analysis data of {symbol} from python-tradingview-ta as following:\n",
    "summary:{summary}\n",
    "oscillators:{oscillators}\n",
    "moving_averages:{moving_averages}\n",
    "indicators:{indicators}\n",
    "\"\"\"\n",
    "        )\n",
    "        PROMPT_TEMPLATE = (\n",
    "            PROMPT_TEMPLATE\n",
    "            + \"\"\"\\nPlease generate the analysis results by analyzing data the above, and provide the market trend.\n",
    "Your generation:\n",
    "\"\"\"\n",
    "        )\n",
    "\n",
    "        prompt = PromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "        return cls(llm=llm, prompt=prompt, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a9891a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from typing import Any, Dict, List, Optional\n",
    "from pydantic import Extra\n",
    "from langchain.base_language import BaseLanguageModel\n",
    "from langchain.callbacks.manager import AsyncCallbackManagerForChainRun,CallbackManagerForChainRun\n",
    "from langchain.chains.base import Chain\n",
    "from langchain.prompts.base import BasePromptTemplate\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "import taapi_docs\n",
    "import cmc_api_docs\n",
    "\n",
    "import os\n",
    "\n",
    "class MarketTrendAndInvestmentAdviseToolChain(Chain):\n",
    "    \"\"\"\n",
    "    An example of a custom chain.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt: BasePromptTemplate\n",
    "    \"\"\"Prompt object to use.\"\"\"\n",
    "    llm: BaseLanguageModel\n",
    "    output_key: str = \"text\"  #: :meta private:\n",
    "\n",
    "    iq_chain:IndicatorsQuestionsChain\n",
    "\n",
    "    rsi_chain : Llama2APIChain\n",
    "    cci_chain : Llama2APIChain\n",
    "    dmi_chain : Llama2APIChain\n",
    "    psar_chain : Llama2APIChain\n",
    "    stochrsi_chain : Llama2APIChain\n",
    "    cmf_chain : Llama2APIChain\n",
    "\n",
    "    latest_quote_chain:Llama2APIChain\n",
    "\n",
    "    mt_chain:MarketTrendChain\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
    "\n",
    "        extra = Extra.forbid\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        \"\"\"Will be whatever keys the prompt expects.\n",
    "\n",
    "        :meta private:\n",
    "        \"\"\"\n",
    "        return self.prompt.input_variables\n",
    "\n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        \"\"\"Will always return text key.\n",
    "\n",
    "        :meta private:\n",
    "        \"\"\"\n",
    "        return [self.output_key]\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        inputs: Dict[str, Any],\n",
    "        run_manager: Optional[CallbackManagerForChainRun] = None,\n",
    "    ) -> Dict[str, str]:\n",
    "        # Your custom chain logic goes here\n",
    "        # This is just an example that mimics LLMChain\n",
    "        prompt_value = self.prompt.format_prompt(**inputs)\n",
    "        if run_manager:\n",
    "             run_manager.on_text(\n",
    "                prompt_value.to_string(), color=\"green\", end=\"\\n\", verbose=self.verbose\n",
    "            )\n",
    "        # Whenever you call a language model, or another chain, you should pass\n",
    "        # a callback manager to it. This allows the inner run to be tracked by\n",
    "        # any callbacks that are registered on the outer run.\n",
    "        # You can always obtain a callback manager for this by calling\n",
    "        # `run_manager.get_child()` as shown below.\n",
    "        response =  self.llm.generate_prompt(\n",
    "            [prompt_value], callbacks=run_manager.get_child() if run_manager else None\n",
    "        )\n",
    "\n",
    "        # If you want to log something about this run, you can do so by calling\n",
    "        # methods on the `run_manager`, as shown below. This will trigger any\n",
    "        # callbacks that are registered for that event.\n",
    "        if run_manager:\n",
    "             run_manager.on_text(\n",
    "                response.generations[0][0].text,\n",
    "                color=\"yellow\",\n",
    "                end=\"\\n\",\n",
    "                verbose=self.verbose,\n",
    "            )\n",
    "        question=response.generations[0][0].text\n",
    "        indicator_questions_json =  self.iq_chain.run(question)\n",
    "        index_questions = json.loads(indicator_questions_json)\n",
    "        \n",
    "        rsi_res =  self.rsi_chain.run(index_questions[\"RSI\"])\n",
    "        cci_res =  self.cci_chain.run(index_questions[\"CCI\"])\n",
    "        dmi_res =  self.dmi_chain.run(index_questions[\"DMI\"])\n",
    "        psar_res =  self.psar_chain.run(index_questions[\"PSAR\"])\n",
    "        stochrsi_res =  self.stochrsi_chain.run(index_questions[\"STOCHRSI\"])\n",
    "        cmf_res =  self.cmf_chain.run(index_questions[\"CMF\"])\n",
    "        latest_quote_res= self.latest_quote_chain.run(question)\n",
    "        \n",
    "        data=rsi_res+\"\\n\"+cci_res+\"\\n\"+dmi_res+\"\\n\"+psar_res+\"\\n\"+stochrsi_res+\"\\n\"+cmf_res+\"\\n\"+latest_quote_res\n",
    "        market_trend_res= self.mt_chain.run(question=question,data=data)\n",
    "\n",
    "        return {self.output_key: market_trend_res}\n",
    "\n",
    "    async def _acall(\n",
    "        self,\n",
    "        inputs: Dict[str, Any],\n",
    "        run_manager: Optional[AsyncCallbackManagerForChainRun] = None,\n",
    "    ) -> Dict[str, str]:\n",
    "        # Your custom chain logic goes here\n",
    "        # This is just an example that mimics LLMChain\n",
    "        prompt_value = self.prompt.format_prompt(**inputs)\n",
    "        if run_manager:\n",
    "            await run_manager.on_text(\n",
    "                prompt_value.to_string(), color=\"green\", end=\"\\n\", verbose=self.verbose\n",
    "            )\n",
    "        # Whenever you call a language model, or another chain, you should pass\n",
    "        # a callback manager to it. This allows the inner run to be tracked by\n",
    "        # any callbacks that are registered on the outer run.\n",
    "        # You can always obtain a callback manager for this by calling\n",
    "        # `run_manager.get_child()` as shown below.\n",
    "        response = await self.llm.agenerate_prompt(\n",
    "            [prompt_value], callbacks=run_manager.get_child() if run_manager else None\n",
    "        )\n",
    "\n",
    "        # If you want to log something about this run, you can do so by calling\n",
    "        # methods on the `run_manager`, as shown below. This will trigger any\n",
    "        # callbacks that are registered for that event.\n",
    "        if run_manager:\n",
    "            await run_manager.on_text(\n",
    "                response.generations[0][0].text,\n",
    "                color=\"yellow\",\n",
    "                end=\"\\n\",\n",
    "                verbose=self.verbose,\n",
    "            )\n",
    "        question=response.generations[0][0].text\n",
    "        indicator_questions_json = await self.iq_chain.arun(question)\n",
    "        index_questions = json.loads(indicator_questions_json)\n",
    "        \n",
    "        rsi_res = await self.rsi_chain.arun(index_questions[\"RSI\"])\n",
    "        cci_res = await self.cci_chain.arun(index_questions[\"CCI\"])\n",
    "        dmi_res = await self.dmi_chain.arun(index_questions[\"DMI\"])\n",
    "        psar_res = await self.psar_chain.arun(index_questions[\"PSAR\"])\n",
    "        stochrsi_res = await self.stochrsi_chain.arun(index_questions[\"STOCHRSI\"])\n",
    "        cmf_res = await self.cmf_chain.arun(index_questions[\"CMF\"])\n",
    "        latest_quote_res=await self.latest_quote_chain.arun(question)\n",
    "\n",
    "        data=rsi_res+\"\\n\"+cci_res+\"\\n\"+dmi_res+\"\\n\"+psar_res+\"\\n\"+stochrsi_res+\"\\n\"+cmf_res+\"\\n\"+latest_quote_res\n",
    "        market_trend_res=await self.mt_chain.arun(question=question,data=data)\n",
    "\n",
    "        return {self.output_key: market_trend_res}\n",
    "\n",
    "    @property\n",
    "    def _chain_type(self) -> str:\n",
    "        return \"market_trend_investment_advise_chain\"\n",
    "\n",
    "    @classmethod\n",
    "    def from_create(\n",
    "        cls,\n",
    "        **kwargs: Any,\n",
    "    ) -> MarketTrendAndInvestmentAdviseToolChain:\n",
    "        prompt_template = (\n",
    "\"\"\"User may ask some cryptocurrency's market trend.\n",
    "Please generate a complete question in English, using user's input below.\n",
    "User's input:{user_input}\n",
    "Complete question:\"\"\"\n",
    ")\n",
    "        content_handler = ContentHandler()\n",
    "        llama2_01 = SagemakerEndpoint(\n",
    "            endpoint_name=\"huggingface-pytorch-tgi-inference-2023-07-24-07-23-15-934\",\n",
    "            # credentials_profile_name=\"default\",\n",
    "            region_name=\"us-east-1\",\n",
    "            model_kwargs={\n",
    "                \"parameters\": {\n",
    "                    \"do_sample\": True,\n",
    "                    # \"top_p\": 0.9,\n",
    "                    # \"top_k\": 10,\n",
    "                    \"repetition_penalty\": 1.03,\n",
    "                    \"max_new_tokens\": 1024,\n",
    "                    \"temperature\": 0.1,\n",
    "                    \"return_full_text\": False,\n",
    "                    # \"max_length\":2048,\n",
    "                    \"truncate\": 2048,\n",
    "                    # \"num_return_sequences\":2000,\n",
    "                    # \"stop\": [\"\\n\"],\n",
    "                },\n",
    "            },\n",
    "            content_handler=content_handler,\n",
    "            verbose=kwargs[\"verbose\"],\n",
    "        )\n",
    "        llama2_09 = SagemakerEndpoint(\n",
    "            endpoint_name=\"huggingface-pytorch-tgi-inference-2023-07-24-07-23-15-934\",\n",
    "            # credentials_profile_name=\"default\",\n",
    "            region_name=\"us-east-1\",\n",
    "            model_kwargs={\n",
    "                \"parameters\": {\n",
    "                    \"do_sample\": True,\n",
    "                    # \"top_p\": 0.9,\n",
    "                    # \"top_k\": 10,\n",
    "                    \"repetition_penalty\": 1.03,\n",
    "                    \"max_new_tokens\": 1024,\n",
    "                    \"temperature\": 0.9,\n",
    "                    \"return_full_text\": False,\n",
    "                    # \"max_length\":2048,\n",
    "                    \"truncate\": 2048,\n",
    "                    # \"num_return_sequences\":2000,\n",
    "                    # \"stop\": [\"\\n\"],\n",
    "                },\n",
    "            },\n",
    "            content_handler=content_handler,\n",
    "            verbose=kwargs[\"verbose\"],\n",
    "        )\n",
    "        prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "        iq_chain = IndicatorsQuestionsChain.from_indicators(\n",
    "            indicators=\"RSI,CCI,DMI,PSAR,STOCHRSI,CMF\", **kwargs, \n",
    "        )\n",
    "\n",
    "        taapi_key = os.getenv(\"TAAPI_KEY\")\n",
    "        headers = {\n",
    "            \"Accepts\": \"application/json\",\n",
    "        }\n",
    "\n",
    "        rsi_api_docs = PromptTemplate.from_template(taapi_docs.RSI_API_DOCS).format(\n",
    "            taapi_key=taapi_key\n",
    "        )\n",
    "        cci_api_docs = PromptTemplate.from_template(taapi_docs.CCI_API_DOCS).format(\n",
    "            taapi_key=taapi_key\n",
    "        )\n",
    "        dmi_api_docs = PromptTemplate.from_template(taapi_docs.DMI_API_DOCS).format(\n",
    "            taapi_key=taapi_key\n",
    "        )\n",
    "        psar_api_docs = PromptTemplate.from_template(taapi_docs.PSAR_API_DOCS).format(\n",
    "            taapi_key=taapi_key\n",
    "        )\n",
    "        stochrsi_api_docs = PromptTemplate.from_template(taapi_docs.STOCHRSI_API_DOCS).format(\n",
    "            taapi_key=taapi_key\n",
    "        )\n",
    "        cmf_api_docs = PromptTemplate.from_template(taapi_docs.CMF_API_DOCS).format(\n",
    "            taapi_key=taapi_key\n",
    "        )\n",
    "        rsi_chain = Llama2APIChain.from_docs(\n",
    "            api_docs=rsi_api_docs, headers=headers,**kwargs, \n",
    "        )\n",
    "        cci_chain = Llama2APIChain.from_docs(\n",
    "            api_docs=cci_api_docs, headers=headers, **kwargs,\n",
    "        )\n",
    "        dmi_chain = Llama2APIChain.from_docs(\n",
    "            api_docs=dmi_api_docs, headers=headers, **kwargs,\n",
    "        )\n",
    "        psar_chain = Llama2APIChain.from_docs(\n",
    "            api_docs=psar_api_docs, headers=headers, **kwargs,\n",
    "        )\n",
    "        stochrsi_chain = Llama2APIChain.from_docs(\n",
    "            api_docs=stochrsi_api_docs, headers=headers, **kwargs,\n",
    "        )\n",
    "        cmf_chain = Llama2APIChain.from_docs(\n",
    "            api_docs=cmf_api_docs, headers=headers, **kwargs,\n",
    "        )\n",
    "\n",
    "        headers = {\n",
    "          'Accepts': 'application/json',\n",
    "          'X-CMC_PRO_API_KEY': os.getenv(\"CMC_API_KEY\"),\n",
    "        }\n",
    "        latest_quote_chain = Llama2APIChain.from_docs(\n",
    "            api_docs=cmc_api_docs.CMC_QUOTE_LASTEST_API_DOC, headers=headers, **kwargs,\n",
    "        )\n",
    "\n",
    "        gpt4 = ChatOpenAI(\n",
    "            model=\"gpt-4\",\n",
    "            temperature=0.9,\n",
    "            **kwargs,\n",
    "        )\n",
    "        mt_chain=MarketTrendChain.from_llm(llm=gpt4,**kwargs)\n",
    "\n",
    "        return cls(\n",
    "            llm=llama2_01,\n",
    "            iq_chain=iq_chain, \n",
    "            rsi_chain=rsi_chain,\n",
    "            cci_chain=cci_chain,\n",
    "            dmi_chain=dmi_chain,\n",
    "            psar_chain=psar_chain,\n",
    "            stochrsi_chain=stochrsi_chain,\n",
    "            cmf_chain=cmf_chain,\n",
    "            latest_quote_chain=latest_quote_chain,\n",
    "            mt_chain=mt_chain,\n",
    "            prompt=prompt, \n",
    "            **kwargs,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88970ab1-e875-487d-b0c8-8d7f47a79c24",
   "metadata": {},
   "source": [
    "swftc(swftcoin) latest quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ec5579",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path=\"env\")\n",
    "\n",
    "mtia_chain = MarketTrendAndInvestmentAdviseToolChain.from_create(verbose=True)\n",
    "user_input = input(\"Your prompt: \")\n",
    "res = mtia_chain.run(user_input)"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
